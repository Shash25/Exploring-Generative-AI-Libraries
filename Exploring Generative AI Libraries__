Transformers
Proposed in a paper titled "Attention Is All You Need" by Vaswani et al. in 2017, the transformer architecture replaced sequential processing with parallel processing. The key component behind its success? The attention mechanism, more precisely, self-attention.

Key steps include:

Tokenization: The first step is breaking down a sentence into tokens (words or subwords).
Embedding: Each token is represented as a vector, capturing its meaning.
Self-attention: The model computes scores determining the importance of every other word for a particular word in the sequence. These scores are used to weight the input tokens and produce a new representation of the sequence. For instance, in the sentence "He gave her a gift because she'd helped him", understanding who "her" refers to requires the model to pay attention to other words in the sentence. The transformer does this for every word, considering the entire context, which is particularly powerful for understanding meaning.
Feed-forward neural networks: After attention, each position is passed through a feed-forward network separately.
Output sequence: The model produces an output sequence, which can be used for various tasks, like classification, translation, or text generation.
Layering: Importantly, transformers are deep models with multiple layers of attention and feed-forward networks, allowing them to learn complex patterns.
The architecture's flexibility has allowed transformers to be used beyond NLP, finding applications in image and video processing too. In NLP, transformer-based models like BERT, GPT, and their variants have set state-of-the-art results in various tasks, from text classification to translation.

Implementation: Building a simple chatbot with transformers
Now, you will build a simple chatbot using transformers library from Hugging Face, which is an open-source natural language processing (NLP) toolkit with many useful features.

Step 1: Installing libraries
!pip install -qq tensorflow
!pip install transformers==4.42.1 -U
!pip install sentencepiece
!pip install torch==2.2.2
!pip install torchtext==0.17.2
!pip install numpy==1.26
#!pip install --upgrade numpy transformers torch

Step 2: Importing the required tools from the transformers library
In the upcoming script, you initiate variables using two invaluable classes from the transformers library:

-model is an instance of the class AutoModelForSeq2SeqLM. This class lets you interact with your chosen language model.
-tokenizer is an instance of the class AutoTokenizer. This class streamlines your input and presents it to the language model in the most efficient manner. It achieves this -by converting your text input into "tokens", which is the model's preferred way of interpreting text. You choose "facebook/blenderbot-400M-distill" for this example model because it is freely available under an open-source license and operates at a relatively brisk pace. For a diverse range of models and their capabilities, you can explore the Hugging Face website: Hugging Face Models.

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Selecting the model. You will be using "facebook/blenderbot-400M-distill" in this example.
model_name = "facebook/blenderbot-400M-distill"

# Load the model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

Following the initialization, let's set up the chat function to enable real-time interaction with the chatbot.
# Define the chat function
def chat_with_bot():
    while True:
        # Get user input
        input_text = input("You: ")

        # Exit conditions
        if input_text.lower() in ["quit", "exit", "bye"]:
            print("Chatbot: Goodbye!")
            break

        # Tokenize input and generate response
        inputs = tokenizer.encode(input_text, return_tensors="pt")
        outputs = model.generate(inputs, max_new_tokens=150) 
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        # Display bot's response
        print("Chatbot:", response)

# Start chatting
chat_with_bot()

Alright! You have successfully interacted with your chatbot. By providing it with a prompt, the chatbot used the power of the transformers library and the underlying model to generate a response. This exemplifies the prowess of transformer-based models in comprehending and generating human-like text based on a given context. As you continue to engage with it, you will observe its capacity to simulate a wide range of conversational topics and styles.


#### Step 3: Trying another language model and comparing the output

You can use a different language model, for example the "[flan-t5-base](https://huggingface.co/google/flan-t5-base)" model from Google, to create a similar chatbot. You can use a chat function similar to the one defined in Step 2 and compare the outputs of both models.

import sentencepiece
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

### Let's chat with another bot
def chat_with_another_bot():
    while True:
        # Get user input
        input_text = input("You: ")

        # Exit conditions
        if input_text.lower() in ["quit", "exit", "bye"]:
            print("Chatbot: Goodbye!")
            break

        # Tokenize input and generate response
        inputs = tokenizer.encode(input_text, return_tensors="pt")
        outputs = model.generate(inputs, max_new_tokens=150) 
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        # Display bot's response
        print("Chatbot:", response)

# Start chatting
chat_with_another_bot()

There are many language models available in Hugging Face. In the following exercise, you will compare the output for the same input using two different models.












